{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7359ea81",
   "metadata": {},
   "source": [
    "# 跟随以下提示复现项目：\n",
    "\n",
    "在开始浮现之前你首先要了解如何更改配置与项目结构，项目结构如下\n",
    "\n",
    "项目目录（推荐为 /home/xilinx/jupyter_notebooks/SGSAM/ ）\n",
    "\n",
    "        ├── experiments/                # experiments目录\n",
    "        │   ├── params.npz              # final recon需要的参数文件\n",
    "        │\n",
    "        ├── utils/                      # utils目录\n",
    "        │   ├── common_helpers.py\n",
    "        │   ├── ip_helpers.py\n",
    "        │   ├── rasterization.py\n",
    "        │   ├── rasterize_helpers.py\n",
    "        │   ├── recon_helpers.py\n",
    "        │   └── slam_helpers.py\n",
    "        │\n",
    "        ├── configs.py        # <---------配置文件，可配置相机拍摄帧数，反向传播迭代数，学习率等参数\n",
    "        ├── iphone_demo.ipynb           # 当前文件\n",
    "        ├── final_recon.ipynb           # final_recon 文件\n",
    "        ├── first_frame.pkl             # 备用方案示例帧 \n",
    "        └── output_img.jpg              # 输出的jpg格式图像\n",
    "\n",
    "\n",
    "1. 要首你需要AMD官方PYNQ镜像来运行Jupyter NoteBook\n",
    "\n",
    "2. 你需将我们提供的vitis_design.bit和vitis_design.hwh文件放置在/home/xilinx/pynq/overlays目录下，例如：在其中创建my_project目录将.bit和.hwh文件复制到目录中，这样就不用更改下方单元格中的代码\n",
    "\n",
    "3. 运行下方单元各即可加载overlay\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pynq\n",
    "from pynq import allocate\n",
    "\n",
    "ol = pynq.Overlay('my_project/vitis_design.bit')\n",
    "ol?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5fba49c",
   "metadata": {},
   "source": [
    "4. 获取依赖包，需要注意的是，原生Pynq镜像不包含torch这个库，在Pynq控制台终端执行以下命令来安装torch\n",
    "\n",
    "        source /etc/profile.d/pynq_venv.sh\n",
    "        pip install --upgrade numpy==1.23.5\n",
    "        source /etc/profile.d/pynq_venv.sh\n",
    "        pip install --user torch==1.13.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d57da4407088f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import time\n",
    "import pickle\n",
    "import struct\n",
    "import asyncio\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from configs import get_config\n",
    "from utils.ip_helpers import *\n",
    "from utils.slam_helpers import *\n",
    "from utils.rasterization import *\n",
    "from utils.recon_helpers import *\n",
    "from utils.common_helpers import *\n",
    "from utils.rasterize_helpers import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70262bbd",
   "metadata": {},
   "source": [
    "5. 安装cyclonedds   ( 来源：https://github.com/eclipse-cyclonedds/cyclonedds )\n",
    "\n",
    "    在Pynq终端输入：\n",
    "\n",
    "        git clone https://github.com/eclipse-cyclonedds/cyclonedds\n",
    "        cd cyclonedds\n",
    "        git checkout 0.10.5\n",
    "        mkdir build install && cd build\n",
    "        cmake .. -DCMAKE_INSTALL_PREFIX=../install\n",
    "        cmake --build . --config RelWithDebInfo --target install\n",
    "        cd ..\n",
    "        export CYCLONEDDS_HOME=\"$(pwd)/install\"\n",
    "        pip3 install cyclonedds==0.10.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab062a7aca553af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cyclonedds.idl as idl\n",
    "import cyclonedds.idl.annotations as annotate\n",
    "import cyclonedds.idl.types as types\n",
    "from dataclasses import dataclass\n",
    "from cyclonedds.domain import DomainParticipant, Domain\n",
    "from cyclonedds.core import Qos, Policy\n",
    "from cyclonedds.sub import DataReader\n",
    "from cyclonedds.topic import Topic\n",
    "from cyclonedds.util import duration\n",
    "\n",
    "@dataclass\n",
    "@annotate.final\n",
    "@annotate.autoid(\"sequential\")\n",
    "class SplatCaptureFrame(idl.IdlStruct, typename=\"SplatCaptureData.SplatCaptureFrame\"):\n",
    "    id: types.uint32\n",
    "    annotate.key(\"id\")\n",
    "    timestamp: types.float64\n",
    "    fl_x: types.float32\n",
    "    fl_y: types.float32\n",
    "    cx: types.float32\n",
    "    cy: types.float32\n",
    "    transform_matrix: types.array[types.float32, 16]\n",
    "    width: types.uint32\n",
    "    height: types.uint32\n",
    "    image: types.sequence[types.uint8]\n",
    "    has_depth: bool\n",
    "    depth_width: types.uint32\n",
    "    depth_height: types.uint32\n",
    "    depth_scale: types.float32\n",
    "    depth_image: types.sequence[types.uint8]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "259cdd2a",
   "metadata": {},
   "source": [
    "6. 在安装完成cyclonedds后，在控制台输入\n",
    "\n",
    "        sudo sysctl -w net.core.rmem_max=2147483647\n",
    "\n",
    "        sudo sysctl -w net.core.wmem_max=2147483647\n",
    "\n",
    "完成对网络读写容量的配置，\n",
    "\n",
    "接下来你需要根据你为zcu104板卡分配的ip地址替换下方单元格中的192.168.xxx.xxx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d054b1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dds_config = \"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\" ?> \\\n",
    "<CycloneDDS xmlns=\"https://cdds.io/config\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"https://cdds.io/config https://raw.githubusercontent.com/eclipse-cyclonedds/cyclonedds/master/etc/cyclonedds.xsd\"> \\\n",
    "    <Domain id=\"any\"> \\\n",
    "        <General> \\\n",
    "            <NetworkInterfaceAddress>192.168.xxx.xxx</NetworkInterfaceAddress> \\\n",
    "        </General> \\\n",
    "        <Internal> \\\n",
    "            <MinimumSocketReceiveBufferSize>10MB</MinimumSocketReceiveBufferSize> \\\n",
    "        </Internal> \\\n",
    "        <Tracing> \\\n",
    "            <Verbosity>config</Verbosity> \\\n",
    "            <OutputFile>stdout</OutputFile> \\\n",
    "        </Tracing> \\\n",
    "    </Domain> \\\n",
    "</CycloneDDS> \\\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642b8936",
   "metadata": {},
   "source": [
    "7. 要运行下方单元格内容，需要将支持 $1920 \\times 1200$ 的DP显示屏连接到zcu104的DisplayPort上，或通过主动式DP转HDMI转换器连接同样规格的HDMI显示屏，并为显示屏通电\n",
    "\n",
    "    如果无此配置的显示屏则需要修改VideoMode(1920, 1200, 24)\n",
    "\n",
    "    如果不想使用显示屏可以删除或备注下面4行\n",
    "\n",
    "        from pynq.lib.video import *\n",
    "        displayport = DisplayPort()\n",
    "        displayport.configure(VideoMode(1920, 1200, 24), PIXEL_RGB)\n",
    "        displayport.start()\n",
    "\n",
    "    无cycloneDDS可以删除或备注下面7行\n",
    "\n",
    "        # Setup DDS\n",
    "        domain = Domain(domain_id=0, config=dds_config)\n",
    "        participant = DomainParticipant()\n",
    "        qos = Qos(Policy.Reliability.Reliable(\n",
    "            max_blocking_time=duration(seconds=1)))\n",
    "        topic = Topic(participant, \"Frames\", SplatCaptureFrame, qos=qos)\n",
    "        reader = DataReader(participant, topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786c9d105fbabca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pynq.lib.video import *\n",
    "displayport = DisplayPort()\n",
    "displayport.configure(VideoMode(1920, 1200, 24), PIXEL_RGB)\n",
    "displayport.start()\n",
    "\n",
    "config = get_config()\n",
    "\n",
    "# Set Seed\n",
    "seed_everything(seed=config['seed'])\n",
    "\n",
    "# Setup DDS\n",
    "domain = Domain(domain_id=0, config=dds_config)\n",
    "participant = DomainParticipant()\n",
    "qos = Qos(Policy.Reliability.Reliable(\n",
    "    max_blocking_time=duration(seconds=1)))\n",
    "topic = Topic(participant, \"Frames\", SplatCaptureFrame, qos=qos)\n",
    "reader = DataReader(participant, topic)\n",
    "\n",
    "# Create Results Directory and Copy Config\n",
    "results_dir = os.path.join(\n",
    "    config[\"workdir\"], config[\"run_name\"]\n",
    ")\n",
    "os.makedirs(results_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3e929b",
   "metadata": {},
   "source": [
    "8. 运行下方单元格来进行建图，在iPhone14以上带TOF传感器的机型上运行 **NeRF Capture** app ，在Waiting for frames...字样出现后拍照，注意手机与板卡应当处在同一局域网下\n",
    "\n",
    "    如果cycloneDDS安装失败，或缺少**NeRF Capture** app ，可以将单元格中的\n",
    "\n",
    "        sample = reader.read_next()\n",
    "\n",
    "    替换为\n",
    "\n",
    "        with open('first_frame.pkl', 'rb') as f:\n",
    "            sample = pickle.load(f)\n",
    "    \n",
    "    确保first_frame.pkl与该NoteBook在同意目录下\n",
    "\n",
    "    如果之前决定不使用显示屏，将下方单元格99~117行备注或删除，具体为以下内容：\n",
    "\n",
    "        im = im.clone().detach().numpy().reshape(360, 480, 3) * 255\n",
    "        im[im > 255] = 255\n",
    "        im[im < 0] = 0\n",
    "        img_uint8 = im.astype(np.uint8)\n",
    "        img_RGB = cv2.cvtColor(img_uint8, cv2.COLOR_BGR2RGB)\n",
    "        # 获取原始图像尺寸\n",
    "        orig_height, orig_width = img_RGB.shape[:2]\n",
    "        orig_height *= 3\n",
    "        orig_width *= 3\n",
    "        img_RGB = cv2.resize(img_RGB, (orig_width,orig_height))\n",
    "        # 创建黑色背景帧\n",
    "        frame = displayport.newframe()\n",
    "        frame[:] = 0  # 填充黑色\n",
    "        # 计算居中位置\n",
    "        start_x = (1920 - orig_width) // 2\n",
    "        start_y = (1200 - orig_height) // 2\n",
    "        # 将图像复制到帧的中央位置\n",
    "        frame[start_y:start_y+orig_height, start_x:start_x+orig_width] = img_RGB\n",
    "        displayport.writeframe(frame)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c91a61a3a74a88",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Waiting for frames...\")\n",
    "total_frames = 0 # Total frames received\n",
    "time_idx = total_frames\n",
    "num_frames = config[\"num_frames\"] # Total frames desired\n",
    "\n",
    "# Init Variables to keep track of ARkit poses and runtimes\n",
    "mapping_iter_time_sum = 0\n",
    "mapping_iter_time_count = 0\n",
    "mapping_frame_time_sum = 0\n",
    "mapping_frame_time_count = 0\n",
    "P = torch.tensor(\n",
    "    [[1, 0, 0, 0],\n",
    "    [0, -1, 0, 0],\n",
    "    [0, 0, -1, 0],\n",
    "    [0, 0, 0, 1]]).float()\n",
    "\n",
    "ip = ol.rasterization_1\n",
    "\n",
    "# Start DDS Loop\n",
    "while True:\n",
    "    sample = reader.read_next()\n",
    "    \n",
    "    if sample:\n",
    "        print(f\"{total_frames + 1}/{num_frames} frames received\")\n",
    "\n",
    "        # RGB & Depth\n",
    "        image = np.asarray(sample.image, dtype=np.uint8).reshape((sample.height, sample.width, 3))\n",
    "        if sample.has_depth:\n",
    "            curr_depth = np.asarray(sample.depth_image, dtype=np.uint8).view(\n",
    "                dtype=np.float32).reshape((sample.depth_height, sample.depth_width))\n",
    "        else:\n",
    "            print(\"No Depth Image Received. Please make sure that the NeRFCapture App \\\n",
    "                  mentions Depth Supported on the top right corner. Skipping Frame...\")\n",
    "            continue\n",
    "\n",
    "        # ARKit Poses for saving dataset & Convert ARKit Pose to GradSLAM format\n",
    "        X_WV = np.asarray(sample.transform_matrix, dtype=np.float32).reshape((4, 4)).T\n",
    "        gt_pose = torch.from_numpy(X_WV).float()\n",
    "        gt_pose = P @ gt_pose @ P.T\n",
    "        gt_pose = gt_pose.unsqueeze(0)\n",
    "        gt_w2c = torch.linalg.inv(gt_pose[0])\n",
    "\n",
    "        # Initialize Mapping Resolution Data\n",
    "        color = cv2.resize(image, dsize=(\n",
    "            config['data']['image_width'], config['data']['image_height']), interpolation=cv2.INTER_LINEAR)\n",
    "        depth = cv2.resize(curr_depth, dsize=(\n",
    "                config['data']['image_width'], config['data']['image_height']), interpolation=cv2.INTER_NEAREST)\n",
    "        depth = np.expand_dims(depth, -1)\n",
    "\n",
    "        color = torch.from_numpy(color).float()\n",
    "        depth = torch.from_numpy(depth).float()\n",
    "        color /= 255\n",
    "\n",
    "        intrinsics = torch.tensor([[sample.fl_x, 0, sample.cx], [0, sample.fl_y, sample.cy], [0, 0, 1]]).float()\n",
    "        intrinsics = intrinsics / config['data']['downscale_factor']\n",
    "        intrinsics[2, 2] = 1.0\n",
    "        mask = (depth > 0) # Mask out invalid depth values\n",
    "        thresholds = method_custom_equal_height(depth[mask], 256)\n",
    "        cam = setup_camera(color.shape[1], color.shape[0], thresholds, intrinsics.cpu().numpy(), gt_w2c.cpu().numpy())\n",
    "\n",
    "        # Initialize Params for first time step\n",
    "        if time_idx == 0:\n",
    "            mask = mask.reshape(-1)\n",
    "            init_pt_cld, mean3_dist = get_pointcloud(color, depth, intrinsics, gt_w2c,\n",
    "                                                        mask=mask, compute_mean_sq_dist=True,\n",
    "                                                        mean_sq_dist_method=config['mean_sq_dist_method'])\n",
    "            params, variables = initialize_params(init_pt_cld, num_frames, mean3_dist)\n",
    "            variables['scene_radius'] = torch.max(depth)/config['scene_radius_depth_ratio']\n",
    "\n",
    "        # Initialize Mapping & Tracking for current frame\n",
    "        iter_time_idx = time_idx\n",
    "        curr_data = {'cam': cam, 'im': color, 'depth': depth, 'id': iter_time_idx, 'curr_w2c': gt_w2c, 'intrinsics':intrinsics }\n",
    "        num_iters_mapping = config['mapping']['num_iters']\n",
    "\n",
    "        # Densification\n",
    "        if time_idx == 0 or (time_idx+1) % config['map_every'] == 0:\n",
    "            if config['mapping']['add_new_gaussians'] and time_idx > 0:\n",
    "                params, variables = add_new_gaussians(ip,params, variables, curr_data,\n",
    "                                                    config['mapping']['sil_thres'], time_idx,\n",
    "                                                    config['mean_sq_dist_method'])\n",
    "\n",
    "        # Reset Optimizer & Learning Rates for Full Map Optimization\n",
    "        optimizer = initialize_optimizer(params, config['mapping']['lrs'])\n",
    "\n",
    "        # Mapping\n",
    "        for iter in range(num_iters_mapping):\n",
    "            iter_start_time = time.time()\n",
    "\n",
    "            # Loss for current frame\n",
    "            loss, variables, losses, im = get_loss(ip, params, curr_data, variables, time_idx, config['mapping']['loss_weights'],\n",
    "                                            config['mapping']['use_sil_for_loss'], config['mapping']['sil_thres'],\n",
    "                                            config['mapping']['use_l1'], config['mapping']['ignore_outlier_depth_loss'])\n",
    "            \n",
    "            get_loss_time = time.time()\n",
    "            # Backprop\n",
    "            loss.backward()\n",
    "            backward_time = time.time()\n",
    "            \n",
    "            im = im.clone().detach().numpy().reshape(360, 480, 3) * 255\n",
    "            im[im > 255] = 255\n",
    "            im[im < 0] = 0\n",
    "            img_uint8 = im.astype(np.uint8)\n",
    "            img_RGB = cv2.cvtColor(img_uint8, cv2.COLOR_BGR2RGB)\n",
    "            # 获取原始图像尺寸\n",
    "            orig_height, orig_width = img_RGB.shape[:2]\n",
    "            orig_height *= 3\n",
    "            orig_width *= 3\n",
    "            img_RGB = cv2.resize(img_RGB, (orig_width,orig_height))\n",
    "            # 创建黑色背景帧\n",
    "            frame = displayport.newframe()\n",
    "            frame[:] = 0  # 填充黑色\n",
    "            # 计算居中位置\n",
    "            start_x = (1920 - orig_width) // 2\n",
    "            start_y = (1200 - orig_height) // 2\n",
    "            # 将图像复制到帧的中央位置\n",
    "            frame[start_y:start_y+orig_height, start_x:start_x+orig_width] = img_RGB\n",
    "            displayport.writeframe(frame)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                # Optimizer Update\n",
    "                optimizer.step() \n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "                \n",
    "            # Update the runtime numbers\n",
    "            iter_end_time = time.time()\n",
    "            mapping_iter_time_sum += iter_end_time - iter_start_time\n",
    "            mapping_iter_time_count += 1\n",
    "\n",
    "        # Update frame count at end\n",
    "        if total_frames == num_frames - 1:\n",
    "            break\n",
    "        total_frames += 1\n",
    "        time_idx = total_frames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e23574",
   "metadata": {},
   "source": [
    "9. 运行下方单元格将保存经过方向传播优化参数后的渲染结果，视角与最后一帧图片相同"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f163a7cc18ef5ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_bgr = cv2.cvtColor(img_uint8, cv2.COLOR_RGB2BGR)\n",
    "cv2.imwrite('output_image.jpg', img_bgr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "768adde4",
   "metadata": {},
   "source": [
    "10. 运行下方单元格保存点云供final recon使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762edc7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "print(curr_data['intrinsics'])\n",
    "\n",
    "params_to_save = {\n",
    "    'means3D': params['means3D'].detach().numpy(),\n",
    "    'rgb_colors': params['rgb_colors'].detach().numpy(),\n",
    "    'logit_opacities': params['logit_opacities'].detach().numpy(),\n",
    "    'scales': torch.exp(params['log_scales']).detach().numpy(),\n",
    "    'width': 480,\n",
    "    'height': 360,\n",
    "    'intrinsics': curr_data['intrinsics'].detach().numpy(),\n",
    "}\n",
    "_BASE_DIR = os.getcwd()\n",
    "sys.path.insert(0, _BASE_DIR)\n",
    "results_dir = os.path.join(_BASE_DIR, \"experiments\")\n",
    "scene_path = os.path.join(results_dir, \"params.npz\")\n",
    "np.savez(scene_path, **params_to_save)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "splatam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
